# -*- coding: utf-8 -*-
"""Meal_detection_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cfBvtNsevyrw86fIfyVgxLXyHIbm7ve9
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import scipy.stats as st
import statsmodels.datasets
import matplotlib.pyplot as plt
import pandas as pd
import csv
import scipy
import pywt
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score

#Importing sklearn libraries for train and test
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm

from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

# %matplotlib inline

def read_csv_file(filepath):
    d = []
    with open(filepath) as csvfile:
        areader = csv.reader(csvfile)
        max_elems = 0
        for row in areader:
            if max_elems < len(row): max_elems = len(row)
        csvfile.seek(0)
        for i, row in enumerate(areader):
            # fix my csv by padding the rows
            d.append(row + ["" for x in range(max_elems-len(row))])

    df = pd.DataFrame(d)
    return df

MealData1 = read_csv_file('./mealData1.csv');
MealData2 = read_csv_file('./mealData2.csv');
MealData3 = read_csv_file('./mealData3.csv');
MealData4 = read_csv_file('./mealData4.csv');
MealData5 = read_csv_file('./mealData5.csv');
#Importing No Meal Data

Nomeal1 = read_csv_file('./Nomeal1.csv')
Nomeal2 = read_csv_file('./Nomeal2.csv')
Nomeal3 = read_csv_file('./Nomeal3.csv')
Nomeal4 = read_csv_file('./Nomeal4.csv')
Nomeal5 = read_csv_file('./Nomeal5.csv')

df_list = [MealData1, MealData2, MealData3, MealData4, MealData5, Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]

print(MealData1.shape, MealData2.shape, MealData3.shape, MealData4.shape, MealData5.shape)
print(Nomeal1.shape, Nomeal2.shape, Nomeal3.shape, Nomeal4.shape, Nomeal5.shape)
print("Total no of records:: ", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0]+MealData4.shape[0]+MealData5.shape[0]
     + Nomeal1.shape[0] + Nomeal2.shape[0] + Nomeal3.shape[0] + Nomeal4.shape[0] + Nomeal5.shape[0])

df_idx = 0
for df in df_list:
    for i in range(len(df.iloc[0])):
        df[i] = pd.to_numeric(df[i],errors='coerce')
    if df_idx == 0:
        MealData1 = df
    if df_idx == 1:
        MealData2 = df
    if df_idx == 2:
        MealData3 = df
    if df_idx == 3:
        MealData4 = df
    if df_idx == 4:
        MealData5 = df
    if df_idx == 5:
        Nomeal1 = df
    if df_idx == 6:
        Nomeal2 = df
    if df_idx == 7:
        Nomeal3 = df
    if df_idx == 8:
        Nomeal4 = df
    if df_idx == 9:
        Nomeal5 = df
    df_list[df_idx] = df
    df_idx+=1

# Drop 31st column

df_idx = 0
for df in df_list:
    if df_idx < 5:
        df = df.drop(df.columns[[30]], axis = 1)
        
        if df_idx == 0:
            MealData1 = df
        if df_idx == 1:
            MealData2 = df
        if df_idx == 2:
            MealData3 = df
        if df_idx == 3:
            MealData4 = df
        if df_idx == 4:
            MealData5 = df
        df_list[df_idx] = df
        
    df_idx+=1

MealData1 = MealData1.iloc[:, ::-1]
MealData2 = MealData2.iloc[:, ::-1]
MealData3 = MealData3.iloc[:, ::-1]
MealData4 = MealData4.iloc[:, ::-1]
MealData5 = MealData5.iloc[:, ::-1]


Nomeal1 = Nomeal1.iloc[:, ::-1]
Nomeal2 = Nomeal2.iloc[:, ::-1]
Nomeal3 = Nomeal3.iloc[:, ::-1]
Nomeal4 = Nomeal4.iloc[:, ::-1]
Nomeal5 = Nomeal5.iloc[:, ::-1]

df_list = [MealData1, MealData2, MealData3, MealData4, MealData5, Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]

df_idx = 0
for df in df_list:
    for i in range(len(df)):
        df.loc[[i]] = df.loc[[i]].interpolate(axis=1, limit=None, limit_direction='both')
    if df_idx == 0:
        MealData1 = df
    if df_idx == 1:
        MealData2 = df
    if df_idx == 2:
        MealData3 = df
    if df_idx == 3:
        MealData4 = df
    if df_idx == 4:
        MealData5 = df
    if df_idx == 5:
        Nomeal1 = df
    if df_idx == 6:
        Nomeal2 = df
    if df_idx == 7:
        Nomeal3 = df
    if df_idx == 8:
        Nomeal4 = df
    if df_idx == 9:
        Nomeal5 = df
    df_list[df_idx] = df
    df_idx+=1

df_idx = 0
for df in df_list:
    df = df.dropna()
    df = df.reset_index(drop=True)
    if df_idx == 0:
        MealData1 = df
    if df_idx == 1:
        MealData2 = df
    if df_idx == 2:
        MealData3 = df
    if df_idx == 3:
        MealData4 = df
    if df_idx == 4:
        MealData5 = df
    if df_idx == 5:
        Nomeal1 = df
    if df_idx == 6:
        Nomeal2 = df
    if df_idx == 7:
        Nomeal3 = df
    if df_idx == 8:
        Nomeal4 = df
    if df_idx == 9:
        Nomeal5 = df
    df_list[df_idx] = df
    df_idx+=1

print(MealData1.shape, MealData2.shape, MealData3.shape, MealData4.shape, MealData5.shape)
print(Nomeal1.shape, Nomeal2.shape, Nomeal3.shape, Nomeal4.shape, Nomeal5.shape)
print("Total no of records:: ", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0]+MealData4.shape[0]+MealData5.shape[0]
     + Nomeal1.shape[0] + Nomeal2.shape[0] + Nomeal3.shape[0] + Nomeal4.shape[0] + Nomeal5.shape[0])

scaler = MinMaxScaler(feature_range=(0,1))

#Scaling CGM Data
MealData1Scaled = scaler.fit_transform(MealData1)
MealData2Scaled = scaler.fit_transform(MealData2)
MealData3Scaled = scaler.fit_transform(MealData3)
MealData4Scaled = scaler.fit_transform(MealData4)
MealData5Scaled = scaler.fit_transform(MealData5)

no_meal1_scaled = scaler.fit_transform(Nomeal1)
no_meal2_scaled = scaler.fit_transform(Nomeal2)
no_meal3_scaled = scaler.fit_transform(Nomeal3)
no_meal4_scaled = scaler.fit_transform(Nomeal4)
no_meal5_scaled = scaler.fit_transform(Nomeal5)

#Feature FFT Calculation
top_fft_features = []

df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,
          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]
i = 0
for df in zip(df_list):
    if len(top_fft_features) == 0:
        top_fft_features = np.abs(np.fft.fft(np.flip(df)))
    else:
        top_fft_features = np.concatenate((top_fft_features, np.abs(np.fft.fft(np.flip(df)))), axis=1)

#DWT Feature Calculation
def calc_feature_dwt(df):
    cA, cB = pywt.dwt(df, 'haar')
    cA_threshold = pywt.threshold(cA, np.std(cA)/2, mode='soft')
    cB_threshold = pywt.threshold(cB, np.std(cB)/2, mode='soft')
 
    
    reconstructed_signal = pywt.idwt(cA_threshold, cB_threshold, 'haar')

    feature_dwt_top8 = cA[:,:-8] #sorted in Ascending

    return feature_dwt_top8

df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,
          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]
top_dwt_features = []
for df in df_list:
    if len(top_dwt_features) == 0:
        top_dwt_features = calc_feature_dwt(df)
    else:
        top_dwt_features = np.concatenate((top_dwt_features, calc_feature_dwt(df)))

# Coeffecient of Variation Feature Calculation
feature_COV = []

df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,
          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]
for df in df_list:
    for i in range(len(df)):
        feature_COV.append(np.mean(df[i]) / np.std(df[i]))

feature_COV_ = np.asarray(feature_COV)

feature_COV_WO_nan = feature_COV_[np.isnan(feature_COV_) == False]
feature_COV_WO_nan.sort()
mean_with_threshold = np.mean(feature_COV_WO_nan[0:len(feature_COV_WO_nan)-1])
mean_with_threshold

# feature_COV_.replace(np.nan,mean_with_threshold)
feature_COV_[feature_COV_ > 200] = mean_with_threshold
for x in range(len(feature_COV_)):
    if np.isnan(feature_COV_[x]):
        feature_COV_[x] = mean_with_threshold
# type(feature_COV_)

from scipy.integrate import simps
feature_auc = []

for df in df_list:
    for x in simps(df[:,::-1], dx = 5):
        feature_auc.append(x)


feature_auc = np.asarray(feature_auc)
feature_auc.shape

#Windowed Entropy Feature 
output_entropy = []

y = []
ordered_cgm = []
df_list = [MealData1, MealData2, MealData3, MealData4, MealData5,
          Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]
for df in df_list:
    for j in range(len(df)):
        temp = []
        temp1 = []
        c = df.iloc[j]
        for m in range(len(c)-1,-1,-1):
            temp.append(c[m])
        y_array = np.array(temp)
        ordered_cgm.append(y_array)
        
for i in range(len(ordered_cgm)):
    entropy_arr = []
    for j in range(1, 30, 5):
        s = scipy.stats.entropy(np.asarray(ordered_cgm)[i, j:j+5])
        #print(s)
        entropy_arr.append(s)
    output_entropy.append(np.amin(np.asarray(entropy_arr)))

output_entropy = np.asarray(output_entropy)
output_entropy.shape

lognorm_mean_list = []
lognorm_std_list = []
df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,
          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]

for df in df_list:
    for i in range(len(df)):
        x = df[i]
        x[x == 0] = np.mean(x)
        mu = np.mean(x) 
        sigma = np.std(x)

        x_exp = x
        mu_exp = np.exp(mu)
        sigma_exp = np.exp(sigma)

        fitting_params_lognormal = scipy.stats.lognorm.fit(x_exp, floc=0, scale=mu_exp)
        lognorm_dist_fitted = scipy.stats.lognorm(*fitting_params_lognormal)
        t = np.linspace(np.min(x_exp), np.max(x_exp), 100)

        lognorm_dist = scipy.stats.lognorm(s=sigma, loc=0, scale=np.exp(mu))
        lognorm_mean_list.append(lognorm_dist.mean())
        lognorm_std_list.append(lognorm_dist.std())


lognorm_std_list = np.asarray(lognorm_std_list)
lognorm_mean_list = np.asarray(lognorm_mean_list)

#Creating the feature Matrix

Feature_Matrix = np.hstack((top_fft_features[0][:,1:9], top_dwt_features[:,1:7], output_entropy[:, None], feature_COV_[:, None],
                            lognorm_mean_list[:,None], lognorm_std_list[:,None], feature_auc[:, None]))
Feature_Matrix.shape

Scaled_Feature_Matrix = scaler.fit_transform(Feature_Matrix)
New_Scaled_Feature_Matrix = np.nan_to_num(Scaled_Feature_Matrix)

Scaled_Feature_Matrix_ = np.asmatrix(New_Scaled_Feature_Matrix)

# Apply PCA for the Feature matrix of Training data
import pickle
pca = PCA(n_components=10)
# Meal-data
reduced_meal_matrix = pca.fit_transform(New_Scaled_Feature_Matrix[0:249])
# Non-meal data
reduced_no_meal_matrix = pca.transform(New_Scaled_Feature_Matrix[249:])

filename = 'pca_model.sav'
pickle.dump(pca,open(filename,'wb'))

Final_Feature_M = np.vstack((reduced_meal_matrix,reduced_no_meal_matrix))
Final_Feature_M.shape
PCA_Feature_Matrix = Final_Feature_M

label_M = []
for i in range(len(reduced_meal_matrix)):
    label_M.append(1)
for i in range(len(reduced_no_meal_matrix)):
    label_M.append(0)
    
label_M_ = np.asarray(label_M)
label_M_.shape

#Array to store accuracy for each K Fold testing
score_knn = []

# confusion matrix
confusion_knn=[]

#storing precision
precision_knn=[]

#recall
recall_knn=[]

#f1 score
f1_score_knn=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

#Creating model with n=5
knn = KNeighborsClassifier(n_neighbors=3)

for train_index, test_index in kf_.split(Final_Feature_M):
    X_train_knn, X_test_knn = Final_Feature_M[train_index], Final_Feature_M[test_index]
    Y_train_knn, Y_test_knn = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    knn.fit(X_train_knn,Y_train_knn)
    preds = knn.predict(X_test_knn)
    
    d=confusion_matrix(Y_test_knn, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_knn, preds)
    precision_knn.append(a)
    
    # Recall
    b=recall_score(Y_test_knn, preds)
    recall_knn.append(b)

    # F1 score
    c=f1_score(Y_test_knn, preds)
    f1_score_knn.append(c)
    
    #Pickle Code Starts
    knn_file = 'knn_model.sav'
    pickle.dump(knn,open(knn_file,'wb'))

    score_knn.append(knn.score(X_test_knn,Y_test_knn))

Final_Accuracy_knn = np.max(score_knn)
Final_precision_knn=np.max(precision_knn)
Final_recall_knn=np.max(recall_knn)
Final_f1_score_knn=np.max(f1_score_knn)
print(result)
print("Accuracy : %s"%(Final_Accuracy_knn))  
print("Precision : %s"%(Final_precision_knn))
print("Recall : %s"%(Final_recall_knn))
print("F1 score : %s"%(Final_f1_score_knn))

# SVM MODEL CODE


#Array to store accuracy for each K Fold testing
score_svm = []

# confusion matrix
confusion_svm=[]

#storing precision
precision_svm=[]

#recall
recall_svm=[]

#f1 score
f1_score_svm=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

#Creating SVM Classifier
svm = svm.SVC(kernel='sigmoid', gamma='auto')

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_svm, X_test_svm = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_svm, Y_test_svm = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    svm.fit(X_train_svm,Y_train_svm)
    preds = svm.predict(X_test_svm)
    
    d=confusion_matrix(Y_test_svm, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_svm, preds)
    precision_svm.append(a)
    
    # Recall
    b=recall_score(Y_test_svm, preds)
    recall_svm.append(b)

    # F1 score
    c=f1_score(Y_test_svm, preds)
    f1_score_svm.append(c)
    
    #Pickle Code Starts
    svm_file = 'svm_model.sav'
    pickle.dump(svm,open(svm_file,'wb'))
    score_svm.append(svm.score(X_test_svm,Y_test_svm))

Final_Accuracy_svm = np.max(score_svm)
Final_precision_svm=np.max(precision_svm)
Final_recall_svm=np.max(recall_svm)
Final_f1_score_svm=np.max(f1_score_svm)
print(result)
print("Accuracy : %s"%(Final_Accuracy_svm))  
print("Precision : %s"%(Final_precision_svm))
print("Recall : %s"%(Final_recall_svm))
print("F1 score : %s"%(Final_f1_score_svm))

# NAIVE BAYES CODE SNIPPET


#Array to store accuracy for each K Fold testing
score_nb = []

# confusion matrix
confusion_nb=[]

#storing precision
precision_nb=[]

#recall
recall_nb=[]

#f1 score
f1_score_nb=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

#Creating Gaussian Classifier
gnb = GaussianNB()

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_nb, X_test_nb = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_nb, Y_test_nb = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    gnb.fit(X_train_nb,Y_train_nb)
    preds = gnb.predict(X_test_nb)
    
    d=confusion_matrix(Y_test_nb, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_nb, preds)
    precision_nb.append(a)
    
    # Recall
    b=recall_score(Y_test_nb, preds)
    recall_nb.append(b)

    # F1 score
    c=f1_score(Y_test_nb, preds)
    f1_score_nb.append(c)
    
    #Pickle Code Starts
    gnb_file = 'gnb_model.sav'
    pickle.dump(gnb,open(gnb_file,'wb'))
    score_nb.append(gnb.score(X_test_nb,Y_test_nb))

Final_Accuracy_nb = np.max(score_nb)
Final_precision_nb=np.max(precision_nb)
Final_recall_nb=np.max(recall_nb)
Final_f1_score_nb=np.max(f1_score_nb)
print(result)
print("Accuracy : %s"%(Final_Accuracy_nb))  
print("Precision : %s"%(Final_precision_nb))
print("Recall : %s"%(Final_recall_nb))
print("F1 score : %s"%(Final_f1_score_nb))

# DECISION TREE CODE

#Array to store Accuracy for each K FOld testing
score_dt = []

# confusion matrix
confusion_dt=[]

#storing precision
precision_dt=[]

#recall
recall_dt=[]

#f1 score
f1_score_dt=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

#Creating Decision Tree Classifier
d_tree = DecisionTreeClassifier()

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    d_tree.fit(X_train_dt,Y_train_dt)
    preds = d_tree.predict(X_test_dt)
    
    d=confusion_matrix(Y_test_dt, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_dt, preds)
    precision_dt.append(a)
    
    # Recall
    b=recall_score(Y_test_dt, preds)
    recall_dt.append(b)

    # F1 score
    c=f1_score(Y_test_dt, preds)
    f1_score_dt.append(c)
    
    #Pickle Code Starts
    d_tree_file = 'd_tree_model.sav'
    pickle.dump(d_tree,open(d_tree_file,'wb'))
    score_dt.append(d_tree.score(X_test_dt,Y_test_dt))

Final_Accuracy_dt = np.max(score_dt)
Final_precision_dt=np.max(precision_dt)
Final_recall_dt=np.max(recall_dt)
Final_f1_score_dt=np.max(f1_score_dt)
print(result)
print("Accuracy : %s"%(Final_Accuracy_dt))  
print("Precision : %s"%(Final_precision_dt))
print("Recall : %s"%(Final_recall_dt))
print("F1 score : %s"%(Final_f1_score_dt))


score_rf = []

# confusion matrix
confusion_rf=[]

#storing precision
precision_rf=[]

#recall
recall_rf=[]

#f1 score
f1_score_rf=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

rand_for = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_rf, X_test_rf = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_rf, Y_test_rf = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    rand_for.fit(X_train_rf,Y_train_rf)
    preds = rand_for.predict(X_test_rf)
    
    d=confusion_matrix(Y_test_rf, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_rf, preds)
    precision_rf.append(a)
    
    # Recall
    b=recall_score(Y_test_rf, preds)
    recall_rf.append(b)

    # F1 score
    c=f1_score(Y_test_rf, preds)
    f1_score_rf.append(c)
    
    
    #Pickle Code Starts
    rand_for_file = 'rand_for_model.sav'
    pickle.dump(rand_for,open(rand_for_file,'wb'))
    
    score_rf.append(rand_for.score(X_test_rf,Y_test_rf))

Final_Accuracy_rf = np.max(score_rf)
Final_precision_rf=np.max(precision_rf)
Final_recall_rf=np.max(recall_rf)
Final_f1_score_rf=np.max(f1_score_rf)
print(result)
print("Accuracy : %s"%(Final_Accuracy_rf))  
print("Precision : %s"%(Final_precision_rf))
print("Recall : %s"%(Final_recall_rf))
print("F1 score : %s"%(Final_f1_score_rf))

#Array to store Accuracy for each K FOld testing
score_dt = []

# confusion matrix
confusion_gb=[]

#storing precision
precision_gb=[]

#recall
recall_gb=[]

#f1 score
f1_score_gb=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_ = KFold(n_splits=10, shuffle=True, random_state=0)

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]
    
    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,
        max_depth=1, random_state=0).fit(X_train_dt, Y_train_dt)
    
    preds = clf.predict(X_test_dt)
    
    d=confusion_matrix(Y_test_dt, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_dt, preds)
    precision_gb.append(a)
    
    # Recall
    b=recall_score(Y_test_dt, preds)
    recall_gb.append(b)

    # F1 score
    c=f1_score(Y_test_dt, preds)
    f1_score_gb.append(c)
    
    
    #Pickle Code Starts
    clf_file = 'clf_model.sav'
    pickle.dump(clf,open(clf_file,'wb'))
    
    score_dt.append(clf.score(X_test_dt, Y_test_dt))
    
Final_Accuracy_gb = np.max(score_dt)
Final_precision_gb=np.max(precision_gb)
Final_recall_gb=np.max(recall_gb)
Final_f1_score_gb=np.max(f1_score_gb)
print(result)
print("Accuracy : %s"%(Final_Accuracy_gb))  
print("Precision : %s"%(Final_precision_gb))
print("Recall : %s"%(Final_recall_gb))
print("F1 score : %s"%(Final_f1_score_gb))


#Array to store Accuracy for each K FOld testing
score_dt1 = []

# confusion matrix
confusion_ab=[]

#storing precision
precision_ab=[]

#recall
recall_ab=[]

#f1 score
f1_score_ab=[]

# result  matrix
d=[]
result = [[0,0],[0,0]]

#Splitting out features and label dataset into train and test sets
kf_1 = KFold(n_splits=10, shuffle=True, random_state=0)

clf1 = AdaBoostClassifier(n_estimators=50, random_state=0)

for train_index, test_index in kf_.split(PCA_Feature_Matrix):
    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]
    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]
    
    #Training model using the train split data from above
    clf1.fit(X_train_dt,Y_train_dt)
    preds = clf1.predict(X_test_dt)
    
    d=confusion_matrix(Y_test_dt, preds)
    
    for i in range(2):
        for j in range(2):
            result[i][j] = (result[i][j] + d[i][j])
    # Precision 
    a=precision_score(Y_test_dt, preds)
    precision_ab.append(a)
    
    # Recall
    b=recall_score(Y_test_dt, preds)
    recall_ab.append(b)

    # F1 score
    c=f1_score(Y_test_dt, preds)
    f1_score_ab.append(c)
    
    
    #Pickle Code Starts
    clf1_file = 'clf1_model.sav'
    pickle.dump(clf1,open(clf1_file,'wb'))
    
    score_dt1.append(clf1.score(X_test_dt,Y_test_dt))

Final_Accuracy_ab = np.max(score_dt1)
Final_precision_ab=np.max(precision_ab)
Final_recall_ab=np.max(recall_ab)
Final_f1_score_ab=np.max(f1_score_ab)
print(result)
print("Accuracy : %s"%(Final_Accuracy_ab))  
print("Precision : %s"%(Final_precision_ab))
print("Recall : %s"%(Final_recall_ab))
print("F1 score : %s"%(Final_f1_score_ab))


# define the keras model
model = Sequential()
model.add(Dense(12, input_dim=10, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit the keras model on the dataset
model.fit(pd.DataFrame(PCA_Feature_Matrix), pd.DataFrame(label_M_),shuffle=True, epochs=200, batch_size=10)
# evaluate the keras model
model.save('ann_model.h5')
_, accuracy = model.evaluate(Final_Feature_M, label_M_)
print('Accuracy: %.2f' % (accuracy*100))

